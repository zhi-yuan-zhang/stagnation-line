{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: Generate Head Dataset and Velocity Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from numba import jit, prange\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Set parameters for the simulation\n",
    "params = {\n",
    "    'HRx': 6, 'HRy': 3, 'HLx': 3, 'HLy': 0,\n",
    "    'LX': 10000, 'LY': 10000, 'D': -2000, 'p': 3, 'q': 3\n",
    "}\n",
    "\n",
    "# Extract parameters from the dictionary\n",
    "HRx, HRy, HLx, HLy = params['HRx'], params['HRy'], params['HLx'], params['HLy']\n",
    "LX, LY, D, p, q = params['LX'], params['LY'], params['D'], params['p'], params['q']\n",
    "\n",
    "# Define the grid resolution\n",
    "mesh_x, mesh_y, mesh_z = 101, 101, 101\n",
    "x_range = np.linspace(0, LX, mesh_x)\n",
    "y_range = np.linspace(0, LY, mesh_y)\n",
    "z_range = np.linspace(D, 0, mesh_z)\n",
    "\n",
    "# Pre-compute constants to optimize performance\n",
    "cosh_D_LX = np.cosh(np.pi * D / LX)\n",
    "cosh_D_LY = np.cosh(np.pi * D / LY)\n",
    "pi_D_LX = np.pi * D / LX\n",
    "pi_D_LY = np.pi * D / LY\n",
    "\n",
    "# Function to calculate the velocity components (u, v, w) and hydraulic head (h)\n",
    "@jit(nopython=True)\n",
    "def compute_velocity(x, y, z, LX, LY, D, HRx, HRy, HLx, HLy, p, q):\n",
    "    # Calculate velocity components u, v, w\n",
    "    u = -(np.pi / LX * HRx * np.sin(np.pi * x / LX) * np.cosh(np.pi * (z - D) / LX) / np.cosh(np.pi * D / LX) +\n",
    "         p * np.pi / LX * HLx * np.sin(p * np.pi * x / LX) * np.cosh(p * np.pi * (z - D) / LX) / np.cosh(p * np.pi * D / LX))\n",
    "    \n",
    "    v = -(np.pi / LY * HRy * np.sin(np.pi * y / LY) * np.cosh(np.pi * (z - D) / LY) / np.cosh(np.pi * D / LY) +\n",
    "         q * np.pi / LY * HLy * np.sin(q * np.pi * y / LY) * np.cosh(q * np.pi * (z - D) / LY) / np.cosh(q * np.pi * D / LY))\n",
    "    \n",
    "    w = (np.pi / LX * HRx * np.cos(np.pi * x / LX) * np.sinh(np.pi * (z - D) / LX) / np.cosh(np.pi * D / LX) +\n",
    "         p * np.pi / LX * HLx * np.cos(p * np.pi * x / LX) * np.sinh(p * np.pi * (z - D) / LX) / np.cosh(p * np.pi * D / LX) +\n",
    "         np.pi / LY * HRy * np.cos(np.pi * y / LY) * np.sinh(np.pi * (z - D) / LY) / np.cosh(np.pi * D / LY) +\n",
    "         q * np.pi / LY * HLy * np.cos(q * np.pi * y / LY) * np.sinh(q * np.pi * (z - D) / LY) / np.cosh(q * np.pi * D / LY))\n",
    "    \n",
    "    # Calculate the hydraulic head h\n",
    "    h = HRx - HRx * np.cos(np.pi * x / LX) * np.cosh(np.pi * (z - D) / LX) / np.cosh(np.pi * D / LX) + \\\n",
    "        HRy - HRy * np.cos(np.pi * y / LY) * np.cosh(np.pi * (z - D) / LY) / np.cosh(np.pi * D / LY) + \\\n",
    "        HLx - HLx * np.cos(p * np.pi * x / LX) * np.cosh(p * np.pi * (z - D) / LX) / np.cosh(p * np.pi * D / LX) + \\\n",
    "        HLy - HLy * np.cos(q * np.pi * y / LY) * np.cosh(q * np.pi * (z - D) / LY) / np.cosh(q * np.pi * D / LY)\n",
    "    \n",
    "    return u, v, w, h\n",
    "\n",
    "# Initialize velocity fields and water table elevation arrays\n",
    "vx = np.zeros((mesh_x, mesh_y, mesh_z))\n",
    "vy = np.zeros((mesh_x, mesh_y, mesh_z))\n",
    "vz = np.zeros((mesh_x, mesh_y, mesh_z))\n",
    "h = np.zeros((mesh_x, mesh_y, mesh_z))\n",
    "zwt = np.zeros((mesh_x, mesh_y))\n",
    "\n",
    "# Function to calculate the velocity field over the entire grid\n",
    "@jit(nopython=True, parallel=True)\n",
    "def calculate_velocity_field(vx, vy, vz, h, zwt, x_range, y_range, z_range, LX, LY, D, HRx, HRy, HLx, HLy, p, q):\n",
    "    for i in prange(mesh_x):\n",
    "        for j in range(mesh_y):\n",
    "            for k in range(mesh_z):\n",
    "                vx[i, j, k], vy[i, j, k], vz[i, j, k], h[i, j, k] = compute_velocity(x_range[i], y_range[j], z_range[k], LX, LY, D, HRx, HRy, HLx, HLy, p, q)\n",
    "            # Calculate the water table elevation (head at z = 0)\n",
    "            zwt[i, j] = h[i, j, -1]\n",
    "\n",
    "# Compute the velocity field\n",
    "calculate_velocity_field(vx, vy, vz, h, zwt, x_range, y_range, z_range, LX, LY, D, HRx, HRy, HLx, HLy, p, q)\n",
    "\n",
    "# Calculate the magnitude of the velocity field\n",
    "UU = np.sqrt(vx**2 + vy**2 + vz**2)\n",
    "\n",
    "# Function to save data to an HDF5 file\n",
    "def save_data_hdf5(filename, vx, vy, vz, h, UU, zwt, x_range, y_range, z_range):\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        f.create_dataset('vx', data=vx)\n",
    "        f.create_dataset('vy', data=vy)\n",
    "        f.create_dataset('vz', data=vz)\n",
    "        f.create_dataset('h', data=h)\n",
    "        f.create_dataset('UU', data=UU)\n",
    "        f.create_dataset('zwt', data=zwt)  # Save zwt\n",
    "        f.create_dataset('x_range', data=x_range)\n",
    "        f.create_dataset('y_range', data=y_range)\n",
    "        f.create_dataset('z_range', data=z_range)\n",
    "\n",
    "# Function to save data to CSV files\n",
    "def save_data_csv(filename, vx, vy, vz, h, UU, zwt, x_range, y_range, z_range):\n",
    "    data = []\n",
    "    for i in range(len(x_range)):\n",
    "        for j in range(len(y_range)):\n",
    "            for k in range(len(z_range)):\n",
    "                data.append([x_range[i], y_range[j], z_range[k], vx[i, j, k], vy[i, j, k], vz[i, j, k], h[i, j, k], UU[i, j, k]])\n",
    "    df = pd.DataFrame(data, columns=['x', 'y', 'z', 'vx', 'vy', 'vz', 'h', 'UU'])\n",
    "    zwt_df = pd.DataFrame(zwt, index=x_range, columns=y_range)\n",
    "    df.to_csv(filename, index=False)\n",
    "    zwt_df.to_csv(filename.replace('.csv', '_zwt.csv'))  # Save zwt as a separate CSV file\n",
    "\n",
    "# Determine the current working directory\n",
    "current_working_dir = os.getcwd()\n",
    "\n",
    "# Define the results folder path\n",
    "result_folder = os.path.join(current_working_dir, 'results')\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "\n",
    "# Define output paths for HDF5 and CSV files\n",
    "output_path_hdf5 = os.path.join(result_folder, 'head_and_velocity.h5')\n",
    "output_path_csv = os.path.join(result_folder, 'head_and_velocity.csv')\n",
    "\n",
    "# Save data to files\n",
    "save_data_hdf5(output_path_hdf5, vx, vy, vz, h, UU, zwt, x_range, y_range, z_range)\n",
    "save_data_csv(output_path_csv, vx, vy, vz, h, UU, zwt, x_range, y_range, z_range)\n",
    "\n",
    "# Create a DataFrame for statistical analysis\n",
    "df = pd.DataFrame({\n",
    "    'vx': vx.flatten(),\n",
    "    'vy': vy.flatten(),\n",
    "    'vz': vz.flatten(),\n",
    "    'h': h.flatten(),\n",
    "    'UU': UU.flatten()\n",
    "})\n",
    "\n",
    "# Set display format to scientific notation with two decimal places\n",
    "pd.options.display.float_format = '{:.2e}'.format\n",
    "\n",
    "# Perform and print statistical description of the data\n",
    "description = df.describe()\n",
    "print(description)\n",
    "\n",
    "# Plot the 3D distribution of the water table elevation (zwt)\n",
    "fig = go.Figure(data=[go.Surface(\n",
    "    z=zwt,\n",
    "    x=x_range,\n",
    "    y=y_range,\n",
    "    colorscale='Rainbow',\n",
    "    colorbar=dict(title='zwt', tickfont=dict(size=12))   \n",
    ")])\n",
    "fig.update_layout(\n",
    "    title='3D Water Table Elevation Distribution',\n",
    "    scene=dict(\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Y',\n",
    "        zaxis_title='Water Table Elevation'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save the plot as an HTML file and display it\n",
    "output_html_path = os.path.join(result_folder, 'Water_table_Case3S.html')\n",
    "fig.write_html(output_html_path)\n",
    "fig.show()\n",
    "\n",
    "def save_data_tecplot(filename, vx, vy, vz, UU, h, x_range, y_range, z_range):\n",
    "    \"\"\"\n",
    "    Saves the velocity components and hydraulic head in a Tecplot-readable DAT file.\n",
    "    \"\"\"\n",
    "    # Create a mesh grid of the coordinates\n",
    "    x, y, z = np.meshgrid(x_range, y_range, z_range, indexing='ij')\n",
    " \n",
    " \n",
    "    # Flatten the arrays\n",
    "    x_flat = x.flatten()\n",
    "    y_flat = y.flatten()\n",
    "    z_flat = z.flatten()\n",
    "    vx_flat = vx.flatten()\n",
    "    vy_flat = vy.flatten()\n",
    "    vz_flat = vz.flatten()\n",
    "    UU_flat = UU.flatten()\n",
    "    h_flat = h.flatten()\n",
    " \n",
    "    # Create a mask for z values greater than or equal to 0\n",
    "    mask = z_flat >= 1\n",
    "    # Stack data column-wise\n",
    "    data = np.column_stack((x_flat, y_flat, z_flat, vx_flat, vy_flat, vz_flat, UU_flat, h_flat))\n",
    "\n",
    "    # Sort data by x, then y, then z\n",
    "    data = data[np.lexsort((z_flat, y_flat, x_flat))]\n",
    "\n",
    "    # Write the header information and data to the file\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('VARIABLES = \"X\", \"Y\", \"Z\", \"VX\", \"VY\", \"VZ\", \"UU\", \"h\"\\n')\n",
    "        f.write('ZONE T=\"3D Flow Field\", I={}, J={}, K={}, F=POINT\\n'.format(mesh_x, mesh_y, mesh_z))\n",
    "        np.savetxt(f, data, fmt='%1.6e', delimiter=', ')\n",
    "\n",
    "# Ensure the call to save_data_tecplot uses the correct parameters\n",
    "output_path_tecplot = os.path.join(result_folder, 'head_and_velocity.dat')\n",
    "save_data_tecplot(output_path_tecplot, vx, vy, vz, UU, h, x_range, y_range, z_range)\n",
    "\n",
    "\n",
    "# Print completion message with the output file paths\n",
    "print(f'Data successfully saved to {output_path_hdf5} and {output_path_csv}')\n",
    "print(f'Tecplot DAT file has been saved to {output_path_tecplot}')\n",
    "print(f'Water table elevation plot saved to {output_html_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26e779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: Finding Out (Pseudo-)stagnation Points/Lines\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import h5py\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.ndimage import minimum_filter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "\n",
    "# Get the current working directory\n",
    "script_dir = Path.cwd()\n",
    "\n",
    "# Construct the data file path relative to the script directory\n",
    "file_relative_path = Path('results', 'head_and_velocity.h5')\n",
    "file_path = script_dir / file_relative_path\n",
    "\n",
    "# Check if the file exists\n",
    "if not file_path.exists():\n",
    "    print(f\"Error: File '{file_path}' does not exist!\")\n",
    "    exit()\n",
    "\n",
    "# Attempt to load the data file\n",
    "try:\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        x_range = f['x_range'][:]\n",
    "        y_range = f['y_range'][:]\n",
    "        z_range = f['z_range'][:]\n",
    "        vx = f['vx'][:]\n",
    "        vy = f['vy'][:]\n",
    "        vz = f['vz'][:]\n",
    "        UU = f['UU'][:]\n",
    "    print(\"File loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define the boundaries for filtering\n",
    "x_min, x_max = 0, 10000\n",
    "y_min, y_max = 0, 10000\n",
    "z_min, z_max = -2000, 0\n",
    "\n",
    "# Create a grid and filter data within the defined range\n",
    "X, Y, Z = np.meshgrid(x_range, y_range, z_range, indexing='ij')\n",
    "filtered_indices = (\n",
    "    (X >= x_min) & (X <= x_max) &\n",
    "    (Y >= y_min) & (Y <= y_max) &\n",
    "    (Z >= z_min) & (Z <= z_max)\n",
    ")\n",
    "X_filtered = X[filtered_indices]\n",
    "Y_filtered = Y[filtered_indices]\n",
    "Z_filtered = Z[filtered_indices]\n",
    "vx_filtered = vx[filtered_indices]\n",
    "vy_filtered = vy[filtered_indices]\n",
    "vz_filtered = vz[filtered_indices]\n",
    "UU_filtered = UU[filtered_indices]\n",
    "\n",
    "# Define a function to filter points that are at least 200 meters apart\n",
    "def filter_points(points):\n",
    "    filtered = []\n",
    "    for point in points:\n",
    "        if all(np.linalg.norm(np.array(point) - np.array(p)) >= 200 for p in filtered):\n",
    "            filtered.append(point)\n",
    "    return filtered\n",
    "\n",
    "# Set thresholds and boundary deltas\n",
    "threshold = 1e-3\n",
    "x_bd, y_bd, z_bd = 0, 0, 0\n",
    "\n",
    "# Define a function to process each y-slice\n",
    "def process_slice_y(y):\n",
    "    slice_indices = np.where(Y_filtered == y)\n",
    "    if len(slice_indices[0]) == 0:\n",
    "        print(f\"No data on the slice at y={y}.\")\n",
    "        return []\n",
    "\n",
    "    points = np.array([X_filtered[slice_indices], Z_filtered[slice_indices]]).T\n",
    "    vx_values = vx_filtered[slice_indices]\n",
    "    vy_values = vy_filtered[slice_indices]\n",
    "    vz_values = vz_filtered[slice_indices]\n",
    "    UU_values = UU_filtered[slice_indices]\n",
    "\n",
    "    grid_x, grid_z = np.mgrid[x_min:x_max:200j, z_min:z_max:200j]\n",
    "\n",
    "    grid_vx = griddata(points, vx_values, (grid_x, grid_z), method='linear')\n",
    "    grid_vy = griddata(points, vy_values, (grid_x, grid_z), method='linear')\n",
    "    grid_vz = griddata(points, vz_values, (grid_x, grid_z), method='linear')\n",
    "    grid_UU = griddata(points, UU_values, (grid_x, grid_z), method='linear')\n",
    "\n",
    "    UU_local_min_points_slice = []\n",
    "\n",
    "    # Detect local minima in the UU grid\n",
    "    UU_local_min = (grid_UU == minimum_filter(grid_UU, size=50))\n",
    "\n",
    "    for i in range(1, grid_x.shape[0] - 1):\n",
    "        for j in range(1, grid_x.shape[1] - 1):\n",
    "            if UU_local_min[i, j] and x_min + x_bd <= grid_x[i, j] <= x_max - x_bd and z_min + z_bd <= grid_z[i, j] <= z_max - z_bd:\n",
    "                if grid_UU[i, j] < threshold:\n",
    "                    UU_local_min_points_slice.append((grid_x[i, j], y, grid_z[i, j], grid_UU[i, j]))\n",
    "\n",
    "    # Filter points that are too close to each other\n",
    "    UU_local_min_points_slice = filter_points(UU_local_min_points_slice)\n",
    "\n",
    "    return UU_local_min_points_slice\n",
    "\n",
    "\n",
    "# Parallel processing of y and x slices\n",
    "y_slices = np.arange(y_min, y_max + 1, 100)\n",
    "\n",
    "UU_local_min_points_y = []\n",
    "\n",
    "\n",
    "# Use ThreadPoolExecutor to speed up processing\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    results_y = executor.map(process_slice_y, y_slices)\n",
    "    for res in results_y:\n",
    "        UU_local_min_points_y.extend(res)\n",
    "\n",
    "# Aggregate and filter points from all slices\n",
    "def compare_and_filter_points(points):\n",
    "    filtered = []\n",
    "    for point in points:\n",
    "        existing_point = next((p for p in filtered if np.linalg.norm(np.array(point[:3]) - np.array(p[:3])) < 1), None)\n",
    "        if existing_point:\n",
    "            if point[3] < existing_point[3]:\n",
    "                filtered.remove(existing_point)\n",
    "                filtered.append(point)\n",
    "        else:\n",
    "            filtered.append(point)\n",
    "    return filtered\n",
    "\n",
    "# Sort points by their x-coordinate\n",
    "def sort_points_by_x(points):\n",
    "    return sorted(points, key=lambda point: point[0])\n",
    "\n",
    "# Save filtered and sorted points to CSV files\n",
    "def save_points_to_csv(points, file_name, header):\n",
    "    csv_path = script_dir / 'results' / file_name\n",
    "    with open(csv_path, 'w') as f:\n",
    "        f.write(header)\n",
    "        for point in points:\n",
    "            f.write(', '.join(map(str, point)) + '\\n')\n",
    "    print(f\"Saved {len(points)} points to {file_name}\")\n",
    "\n",
    "\n",
    "\n",
    "# Save results to CSV files\n",
    "save_points_to_csv(UU_local_min_points_y, 'best_y.csv', 'x, y, z, UU\\n')\n",
    "\n",
    "# Generate DAT files from CSV files for visualization\n",
    "def process_csv_file(file_prefix):\n",
    "    input_path = os.path.join('results', f'{file_prefix}.csv')\n",
    "    df = pd.read_csv(input_path)\n",
    "    \n",
    "    # Ensure column names are consistent\n",
    "    df.columns = ['x', 'y', 'z', df.columns[-1]]\n",
    "    \n",
    "    output_path_dat = os.path.join('results', f'{file_prefix}.dat')\n",
    "    df[['x', 'y', 'z']].to_csv(output_path_dat, sep='\\t', index=False, header=False)\n",
    "    with open(output_path_dat, 'r+') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write('VARIABLES = \"X\", \"Y\", \"Z\"\\n')\n",
    "        f.write(f\"ZONE T='{file_prefix}'\\n\")\n",
    "        f.write(f'I={len(df)}, J=1, K=1, F=POINT\\n')\n",
    "        f.write(content)\n",
    "    \n",
    "    print(f\"Generated {file_prefix}.dat files\")\n",
    "   \n",
    "   \n",
    "# Process the generated CSV files\n",
    "process_csv_file('best_y')\n",
    "\n",
    "# Visualize the data using Plotly\n",
    "def visualize_data(csv_file, title):\n",
    "    df = pd.read_csv(os.path.join('results', csv_file), skipinitialspace=True)\n",
    "\n",
    "    fig = go.Figure(data=go.Scatter3d(\n",
    "        x=df['x'],\n",
    "        y=df['y'],\n",
    "        z=df['z'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=df['UU'],   # Color by the UU value\n",
    "            colorscale='Rainbow', # Use a rainbow color scale\n",
    "            opacity=0.8\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Update layout to expand display range\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(range=[x_min, x_max], title='X Axis'),\n",
    "            yaxis=dict(range=[y_min, y_max], title='Y Axis'),\n",
    "            zaxis=dict(range=[z_min, z_max], title='Z Axis'),\n",
    "            aspectmode='manual',\n",
    "            aspectratio=dict(x=1, y=1, z=0.5)\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=0)\n",
    "    )\n",
    "    \n",
    "    output_html_path = os.path.join('results', f'{csv_file.split(\".\")[0]}.html')\n",
    "    fig.write_html(output_html_path)\n",
    "    print(f\"Visualization saved as {output_html_path}\")\n",
    "\n",
    "# Generate and visualize plots for each CSV file\n",
    "visualize_data('best_y.csv', 'Local Minima in Y Slices')\n",
    "\n",
    "\n",
    "print(\"All processes completed successfully.\")\n",
    "# Generate visualizations of points\n",
    "def plot_points(points, title, color):\n",
    "    fig = go.Figure(data=go.Scatter3d(\n",
    "        x=[x for x, _, _, _ in points],\n",
    "        y=[y for _, y, _, _ in points],\n",
    "        z=[z for _, _, z, _ in points],\n",
    "        mode='markers',\n",
    "        marker=dict(size=2, color=color),\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout to expand display range\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(range=[x_min, x_max], title='X Axis'),\n",
    "            yaxis=dict(range=[y_min, y_max], title='Y Axis'),\n",
    "            zaxis=dict(range=[z_min, z_max], title='Z Axis'),\n",
    "            aspectmode='manual',\n",
    "            aspectratio=dict(x=1, y=1, z=0.5)\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=0)\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Show and save the plots. \n",
    "print(\"Generating visualization for 'best_y.csv'...\")\n",
    "fig_best_y = plot_points(UU_local_min_points_y, 'Best Points from Y Slices with Local Min UU', 'blue')\n",
    "fig_best_y.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a836f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3-1: Generate Critical Points Around (Pseudo-)Stagnation Points along 6 directions\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Function to process a CSV file, convert it, and generate surrounding points\n",
    "def process_csv_file(file_prefix):\n",
    "    # Define file paths\n",
    "    current_working_dir = os.getcwd()\n",
    "    result_folder = os.path.join(current_working_dir, 'results')\n",
    "    input_path = os.path.join(result_folder, f'{file_prefix}.csv')\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    # Check and clean column names\n",
    "    print(f\"Column names ({file_prefix}):\", df.columns)\n",
    "    df.columns = df.columns.str.strip()  # Remove any leading/trailing spaces from column names\n",
    "\n",
    "    # Ensure the necessary columns are present\n",
    "    required_columns = {'x', 'y', 'z'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise KeyError(f\"Missing required columns: {required_columns - set(df.columns)}\")\n",
    "\n",
    "    # Convert CSV to Tecplot (.dat) format\n",
    "    output_path_dat = os.path.join(result_folder, f'{file_prefix}.dat')\n",
    "    with open(output_path_dat, 'w') as f:\n",
    "        f.write('VARIABLES = \"X\", \"Y\", \"Z\"\\n')\n",
    "        f.write(f\"ZONE T='{file_prefix}'\\n\")\n",
    "        f.write(f'I={len(df)}, J=1, K=1, F=POINT\\n')\n",
    "        df[['x', 'y', 'z']].to_csv(f, sep=' ', index=False, header=False)\n",
    "    print(f'File generated: {output_path_dat}')\n",
    "\n",
    "    # Function to generate surrounding points and save as .csv and .dat files\n",
    "    def generate_surrounding_points(df, filename_prefix, delta_x_neg, delta_x_pos, delta_y_neg, delta_y_pos, delta_z_neg, delta_z_pos):\n",
    "        def write_to_dat(df, output_path, zone_title):\n",
    "            with open(output_path, 'w') as f:\n",
    "                f.write('VARIABLES = \"X\", \"Y\", \"Z\"\\n')\n",
    "                f.write(f\"ZONE T='{zone_title}'\\n\")\n",
    "                f.write(f'I={len(df)}, J=1, K=1, F=POINT\\n')\n",
    "                df[['x', 'y', 'z']].to_csv(f, sep=' ', index=False, header=False)\n",
    "\n",
    "        def create_and_save_copy(df, coordinate, delta, direction):\n",
    "            df_copy = df.copy()\n",
    "            df_copy[coordinate] += delta if direction == '+' else -delta\n",
    "            csv_filename = f'{filename_prefix}_trace_{coordinate}{direction}'\n",
    "            csv_path = os.path.join(result_folder, f'{csv_filename}.csv')\n",
    "            dat_path = os.path.join(result_folder, f'{csv_filename}.dat')\n",
    "            df_copy.to_csv(csv_path, index=False)\n",
    "            write_to_dat(df_copy, dat_path, csv_filename)\n",
    "            print(f'Files generated: {csv_path}, {dat_path}')\n",
    "\n",
    "        # Generate surrounding points for x, y, z directions\n",
    "        create_and_save_copy(df, 'x', delta_x_neg, '-')\n",
    "        create_and_save_copy(df, 'x', delta_x_pos, '+')\n",
    "        create_and_save_copy(df, 'y', delta_y_neg, '-')\n",
    "        create_and_save_copy(df, 'y', delta_y_pos, '+')\n",
    "        create_and_save_copy(df, 'z', delta_z_neg, '-')\n",
    "        create_and_save_copy(df, 'z', delta_z_pos, '+')\n",
    "\n",
    "    # Generate and save surrounding points\n",
    "    generate_surrounding_points(df, file_prefix, 300, 300, 300, 300, 200, 200)\n",
    "\n",
    "    # Define paths for CSV and macro files\n",
    "    csv_files = [\n",
    "        os.path.join(result_folder, f'{file_prefix}_trace_x-.csv'),\n",
    "        os.path.join(result_folder, f'{file_prefix}_trace_x+.csv'),\n",
    "        os.path.join(result_folder, f'{file_prefix}_trace_y-.csv'),\n",
    "        os.path.join(result_folder, f'{file_prefix}_trace_y+.csv'),\n",
    "        os.path.join(result_folder, f'{file_prefix}_trace_z-.csv'),\n",
    "        os.path.join(result_folder, f'{file_prefix}_trace_z+.csv')\n",
    "    ]\n",
    "    macro_files = [\n",
    "        os.path.join(result_folder, f'{file_prefix}_trace_x-.mcr'),\n",
    "        os.path.join(result_folder, f'{file_prefix}_trace_x+.mcr'),\n",
    "        os.path.join(result_folder, f'{file_prefix}_trace_y-.mcr'),\n",
    "        os.path.join(result_folder, f'{file_prefix}_trace_y+.mcr'),\n",
    "        os.path.join(result_folder, f'{file_prefix}_trace_z-.mcr'),\n",
    "        os.path.join(result_folder, f'{file_prefix}_trace_z+.mcr')\n",
    "    ]\n",
    "\n",
    "    # Define streamtrace colors\n",
    "    colors = ['Red', 'Blue', 'Green', 'Yellow', 'Purple', 'Cyan']\n",
    "\n",
    "    # Function to generate macro files for Tecplot\n",
    "    def generate_macro_file(csv_file_path, macro_file_path, color):\n",
    "        points = []\n",
    "        with open(csv_file_path, mode='r') as csvfile, open(macro_file_path, mode='w') as macrofile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                points.append((float(row['x']), float(row['y']), float(row['z'])))\n",
    "            macrofile.write('#!MC 1410\\n')\n",
    "            macrofile.write(f'$!StreamAttributes Color = {color}\\n')\n",
    "            for point in points:\n",
    "                macrofile.write('$!Streamtrace Add\\n')\n",
    "                macrofile.write('  StreamType = VolumeLine\\n')\n",
    "                macrofile.write('  StreamDirection = Both\\n')\n",
    "                macrofile.write('  StartPos\\n')\n",
    "                macrofile.write('    {\\n')\n",
    "                macrofile.write(f'    X = {point[0]}\\n')\n",
    "                macrofile.write(f'    Y = {point[1]}\\n')\n",
    "                macrofile.write(f'    Z = {point[2]}\\n')\n",
    "                macrofile.write('    }\\n')\n",
    "            macrofile.write('$!REDRAWALL\\n')\n",
    "        print(f'Macro file generated: {macro_file_path}')\n",
    "\n",
    "    # Generate macro files for each direction\n",
    "    for csv_file, macro_file, color in zip(csv_files, macro_files, colors):\n",
    "        generate_macro_file(csv_file, macro_file, color)\n",
    "\n",
    "    print(f'Macro files generated: {\", \".join([os.path.basename(file) for file in macro_files])}')\n",
    "\n",
    "# List of file prefixes to process\n",
    "file_prefixes = ['best_y']\n",
    "\n",
    "# Process each file prefix\n",
    "for prefix in file_prefixes:\n",
    "    process_csv_file(prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841033b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Step 3-2: Generate Diving Streamlines Across Critical Points\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.integrate import solve_ivp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from numba import jit\n",
    "\n",
    "# Function to read data from an HDF5 file\n",
    "def read_data_hdf5(filename):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        vx = f['vx'][:]\n",
    "        vy = f['vy'][:]\n",
    "        vz = f['vz'][:]\n",
    "        UU = f['UU'][:]\n",
    "        x_range = f['x_range'][:]\n",
    "        y_range = f['y_range'][:]\n",
    "        z_range = f['z_range'][:]\n",
    "    return vx, vy, vz, UU, x_range, y_range, z_range\n",
    "\n",
    "# JIT-compiled velocity field function using Numba for performance optimization\n",
    "@jit(nopython=True)\n",
    "def velocity_field_numba(pos, vx, vy, vz, x_range, y_range, z_range):\n",
    "    x, y, z = pos\n",
    "    # Check if the point is outside the defined ranges\n",
    "    if x < x_range[0] or x > x_range[-1] or y < y_range[0] or y > y_range[-1] or z < z_range[0] or z > z_range[-1]:\n",
    "        return np.array([0.0, 0.0, 0.0])\n",
    "    \n",
    "    # Locate the index positions for interpolation\n",
    "    xi = np.searchsorted(x_range, x) - 1\n",
    "    yi = np.searchsorted(y_range, y) - 1\n",
    "    zi = np.searchsorted(z_range, z) - 1\n",
    "\n",
    "    # Retrieve the boundary points for interpolation\n",
    "    x1, x2 = x_range[xi], x_range[xi+1]\n",
    "    y1, y2 = y_range[yi], y_range[yi+1]\n",
    "    z1, z2 = z_range[zi], z_range[zi+1]\n",
    "\n",
    "    # Calculate the interpolation factors\n",
    "    xd = (x - x1) / (x2 - x1)\n",
    "    yd = (y - y1) / (y2 - y1)\n",
    "    zd = (z - z1) / (z2 - z1)\n",
    "\n",
    "    # Trilinear interpolation for velocity components\n",
    "    vx_val = ((vx[xi, yi, zi] * (1 - xd) + vx[xi + 1, yi, zi] * xd) * (1 - yd) + \n",
    "              (vx[xi, yi + 1, zi] * (1 - xd) + vx[xi + 1, yi + 1, zi] * xd) * yd) * (1 - zd) + \\\n",
    "             ((vx[xi, yi, zi + 1] * (1 - xd) + vx[xi + 1, yi, zi + 1] * xd) * (1 - yd) + \n",
    "              (vx[xi, yi + 1, zi + 1] * (1 - xd) + vx[xi + 1, yi + 1, zi + 1] * xd) * yd) * zd\n",
    "    \n",
    "    vy_val = ((vy[xi, yi, zi] * (1 - xd) + vy[xi + 1, yi, zi] * xd) * (1 - yd) + \n",
    "              (vy[xi, yi + 1, zi] * (1 - xd) + vy[xi + 1, yi + 1, zi] * xd) * yd) * (1 - zd) + \\\n",
    "             ((vy[xi, yi, zi + 1] * (1 - xd) + vy[xi + 1, yi, zi + 1] * xd) * (1 - yd) + \n",
    "              (vy[xi, yi + 1, zi + 1] * (1 - xd) + vy[xi + 1, yi + 1, zi + 1] * xd) * yd) * zd\n",
    "    \n",
    "    vz_val = ((vz[xi, yi, zi] * (1 - xd) + vz[xi + 1, yi, zi] * xd) * (1 - yd) + \n",
    "              (vz[xi, yi + 1, zi] * (1 - xd) + vz[xi + 1, yi + 1, zi] * xd) * yd) * (1 - zd) + \\\n",
    "             ((vz[xi, yi, zi + 1] * (1 - xd) + vz[xi + 1, yi, zi + 1] * xd) * (1 - yd) + \n",
    "              (vz[xi, yi + 1, zi + 1] * (1 - xd) + vz[xi + 1, yi + 1, zi + 1] * xd) * yd) * zd\n",
    "\n",
    "    return np.array([vx_val, vy_val, vz_val])\n",
    "\n",
    "# Function to compute streamlines given a set of starting points\n",
    "def compute_streamlines(vx, vy, vz, x_range, y_range, z_range, start_points, max_distance=1e7, tol=1e-5):\n",
    "    # Define the velocity field function for the integrator\n",
    "    def velocity_field(t, pos):\n",
    "        return velocity_field_numba(pos, vx, vy, vz, x_range, y_range, z_range)\n",
    "\n",
    "    # Event to stop integration when a streamline hits the surface z = 0\n",
    "    def boundary_event(t, pos):\n",
    "        return pos[2] + 0\n",
    "    \n",
    "    boundary_event.terminal = True\n",
    "    boundary_event.direction = 0\n",
    "\n",
    "    # Function to integrate a single streamline\n",
    "    def integrate_streamline(start):\n",
    "        # Integrate forward in time\n",
    "        result_forward = solve_ivp(velocity_field, [0, max_distance], start, method='RK45', rtol=tol, atol=tol, events=boundary_event)\n",
    "        # Integrate backward in time\n",
    "        result_backward = solve_ivp(velocity_field, [0, -max_distance], start, method='RK45', rtol=tol, atol=tol, events=boundary_event)\n",
    "        # Combine forward and backward trajectories\n",
    "        streamline = np.vstack((result_backward.y.T[::-1], result_forward.y.T))\n",
    "        return streamline\n",
    "    \n",
    "    # Parallelize the integration of multiple streamlines\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        streamlines = list(executor.map(integrate_streamline, start_points))\n",
    "    \n",
    "    return streamlines\n",
    "\n",
    "# Function to plot streamlines and starting points in a 3D space\n",
    "def plot_streamlines_and_points(fig, streamlines, start_points, color):\n",
    "    # Plot each streamline\n",
    "    for streamline in streamlines:\n",
    "        if len(streamline) > 0:\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=streamline[:, 0], y=streamline[:, 1], z=streamline[:, 2],\n",
    "                mode='lines',\n",
    "                line=dict(color=color, width=4)  # Set streamline color and width\n",
    "            ))\n",
    "    \n",
    "    # Plot the starting points\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=start_points[:, 0], y=start_points[:, 1], z=start_points[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(color=color, size=5, symbol='square'),\n",
    "        name=f'Start Points {color}'\n",
    "    ))\n",
    "\n",
    "# Function to read starting points from a CSV file\n",
    "def read_trace_points(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df[['x', 'y', 'z']].values\n",
    "\n",
    "# Get the current working directory\n",
    "current_working_dir = os.getcwd()\n",
    "\n",
    "# Define the result folder path\n",
    "result_folder = os.path.join(current_working_dir, 'results')\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "\n",
    "# Define the input HDF5 file path\n",
    "input_path_hdf5 = os.path.join(result_folder, 'head_and_velocity.h5')\n",
    "\n",
    "# Read data from the HDF5 file\n",
    "vx, vy, vz, UU, x_range, y_range, z_range = read_data_hdf5(input_path_hdf5)\n",
    "\n",
    "# Define paths to the trace point CSV files\n",
    "trace_point_files = [\n",
    "    os.path.join(result_folder, 'best_y_trace_x-.csv'),\n",
    "    os.path.join(result_folder, 'best_y_trace_x+.csv'),    \n",
    "    os.path.join(result_folder, 'best_y_trace_y-.csv'),\n",
    "    os.path.join(result_folder, 'best_y_trace_y+.csv'),\n",
    "    os.path.join(result_folder, 'best_y_trace_z-.csv'),\n",
    "    os.path.join(result_folder, 'best_y_trace_z+.csv')\n",
    "]\n",
    "\n",
    "# Define a list of colors for the streamlines\n",
    "colors = ['darkgreen', 'darkviolet', 'lime', 'magenta', 'blue', 'red']\n",
    "\n",
    "# Plot streamlines for each trace point file\n",
    "for file_path, color in zip(trace_point_files, colors):\n",
    "    # Create a new figure for each trace point file\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Read starting points\n",
    "    start_points = read_trace_points(file_path)\n",
    "    \n",
    "    # Compute streamlines\n",
    "    streamlines = compute_streamlines(vx, vy, vz, x_range, y_range, z_range, start_points)\n",
    "    \n",
    "    # Plot streamlines and starting points\n",
    "    plot_streamlines_and_points(fig, streamlines, start_points, color)\n",
    "\n",
    "    # Update the layout to adjust the axis ranges and aspect ratio\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(range=[0, 10000], title='X Axis'),\n",
    "            yaxis=dict(range=[0, 10000], title='Y Axis'),\n",
    "            zaxis=dict(range=[-2000, 0], title='Z Axis'),\n",
    "            aspectmode='cube'  # Maintain aspect ratio for all axes\n",
    "        ),\n",
    "        title=f'Streamlines for {os.path.basename(file_path)}',\n",
    "        showlegend=False\n",
    "    )\n",
    "\n",
    "    # Define the output file path for the HTML visualization\n",
    "    output_html_path = os.path.join(result_folder, f'{os.path.basename(file_path).replace(\".csv\", \"\")}_streamlines.html')\n",
    "\n",
    "    # Display the figure\n",
    "    fig.show()\n",
    "\n",
    "    # Save the figure as an HTML file\n",
    "    fig.write_html(output_html_path)\n",
    "\n",
    "    # Printf-style output for current processing file\n",
    "    print(f\"Processed file: {file_path} with color {color}\")\n",
    "\n",
    "# Final notification after processing all files\n",
    "print(\"Streamline computation and visualization completed for all trace point files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea8802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Merge Dividing Streamlines and Show Groundwater Flow Systems \n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from scipy.integrate import solve_ivp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from numba import jit\n",
    "\n",
    "# Function to read data from an HDF5 file\n",
    "def read_data_hdf5(filename):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        vx = f['vx'][:]\n",
    "        vy = f['vy'][:]\n",
    "        vz = f['vz'][:]\n",
    "        UU = f['UU'][:]\n",
    "        x_range = f['x_range'][:]\n",
    "        y_range = f['y_range'][:]\n",
    "        z_range = f['z_range'][:]\n",
    "    return vx, vy, vz, UU, x_range, y_range, z_range\n",
    "\n",
    "# JIT-compiled velocity field function\n",
    "@jit(nopython=True)\n",
    "def velocity_field_numba(pos, vx, vy, vz, x_range, y_range, z_range):\n",
    "    x, y, z = pos\n",
    "    if not (x_range[0] <= x <= x_range[-1]) or not (y_range[0] <= y <= y_range[-1]) or not (z_range[0] <= z <= z_range[-1]):\n",
    "        return np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "    xi = np.searchsorted(x_range, x) - 1\n",
    "    yi = np.searchsorted(y_range, y) - 1\n",
    "    zi = np.searchsorted(z_range, z) - 1\n",
    "\n",
    "    xd = (x - x_range[xi]) / (x_range[xi + 1] - x_range[xi])\n",
    "    yd = (y - y_range[yi]) / (y_range[yi + 1] - y_range[yi])\n",
    "    zd = (z - z_range[zi]) / (z_range[zi + 1] - z_range[zi])\n",
    "\n",
    "    vx_val = ((vx[xi, yi, zi] * (1 - xd) + vx[xi + 1, yi, zi] * xd) * (1 - yd) + \n",
    "              (vx[xi, yi + 1, zi] * (1 - xd) + vx[xi + 1, yi + 1, zi] * xd) * yd) * (1 - zd) + \\\n",
    "             ((vx[xi, yi, zi + 1] * (1 - xd) + vx[xi + 1, yi, zi + 1] * xd) * (1 - yd) + \n",
    "              (vx[xi, yi + 1, zi + 1] * (1 - xd) + vx[xi + 1, yi + 1, zi + 1] * xd) * yd) * zd\n",
    "\n",
    "    vy_val = ((vy[xi, yi, zi] * (1 - xd) + vy[xi + 1, yi, zi] * xd) * (1 - yd) + \n",
    "              (vy[xi, yi + 1, zi] * (1 - xd) + vy[xi + 1, yi + 1, zi] * xd) * yd) * (1 - zd) + \\\n",
    "             ((vy[xi, yi, zi + 1] * (1 - xd) + vy[xi + 1, yi, zi + 1] * xd) * (1 - yd) + \n",
    "              (vy[xi, yi + 1, zi + 1] * (1 - xd) + vy[xi + 1, yi + 1, zi + 1] * xd) * yd) * zd\n",
    "\n",
    "    vz_val = ((vz[xi, yi, zi] * (1 - xd) + vz[xi + 1, yi, zi] * xd) * (1 - yd) + \n",
    "              (vz[xi, yi + 1, zi] * (1 - xd) + vz[xi + 1, yi + 1, zi] * xd) * yd) * (1 - zd) + \\\n",
    "             ((vz[xi, yi, zi + 1] * (1 - xd) + vz[xi + 1, yi, zi + 1] * xd) * (1 - yd) + \n",
    "              (vz[xi, yi + 1, zi + 1] * (1 - xd) + vz[xi + 1, yi + 1, zi + 1] * xd) * yd) * zd\n",
    "\n",
    "    return np.array([vx_val, vy_val, vz_val])\n",
    "\n",
    "# Function to compute streamlines\n",
    "def compute_streamlines(vx, vy, vz, x_range, y_range, z_range, start_points, max_distance=1e7, tol=1e-5):\n",
    "    def velocity_field(t, pos):\n",
    "        return velocity_field_numba(pos, vx, vy, vz, x_range, y_range, z_range)\n",
    "\n",
    "    def boundary_event(t, pos):\n",
    "        return pos[2] + 0  # Termination condition, z = 0 (surface)\n",
    "    \n",
    "    boundary_event.terminal = True\n",
    "    boundary_event.direction = 0\n",
    "\n",
    "    def integrate_streamline(start):\n",
    "        result_forward = solve_ivp(velocity_field, [0, max_distance], start, method='RK45', rtol=tol, atol=tol, events=boundary_event)\n",
    "        result_backward = solve_ivp(velocity_field, [0, -max_distance], start, method='RK45', rtol=tol, atol=tol, events=boundary_event)\n",
    "        \n",
    "        streamline = np.vstack((result_backward.y.T[::-1], result_forward.y.T))\n",
    "        return streamline\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        streamlines = list(executor.map(integrate_streamline, start_points))\n",
    "    \n",
    "    return streamlines\n",
    "\n",
    "# Function to plot streamlines and trace points\n",
    "def plot_streamlines_and_points(fig, streamlines, start_points, color):\n",
    "    for streamline in streamlines:\n",
    "        if len(streamline) > 0:\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=streamline[:, 0], y=streamline[:, 1], z=streamline[:, 2],\n",
    "                mode='lines',\n",
    "                line=dict(color=color, width=4)  # Set streamline color and width\n",
    "            ))\n",
    "    \n",
    "    # Plot trace points\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=start_points[:, 0], y=start_points[:, 1], z=start_points[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(color=color, size=5, symbol='square'),\n",
    "        name=f'Start Points {color}'\n",
    "    ))\n",
    "\n",
    "# Function to read trace points from CSV\n",
    "def read_trace_points(file_path):\n",
    "    df = pd.read_csv(file_path, nrows=5)\n",
    "    \n",
    "    # Check the number of columns and read the complete file accordingly\n",
    "    if df.shape[1] == 4:\n",
    "        df = pd.read_csv(file_path, names=['x', 'y', 'z', 'UU'], header=0, skiprows=1)\n",
    "    elif df.shape[1] == 3:\n",
    "        df = pd.read_csv(file_path, names=['x', 'y', 'z'], header=0, skiprows=1)\n",
    "    else:\n",
    "        raise ValueError(\"CSV file format is incorrect, should contain 3 or 4 columns\")\n",
    "    \n",
    "    return df[['x', 'y', 'z']].values\n",
    "\n",
    "# Define paths and directories\n",
    "current_working_dir = os.getcwd()\n",
    "result_folder = os.path.join(current_working_dir, 'results')\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "input_path_hdf5 = os.path.join(result_folder, 'head_and_velocity.h5')\n",
    "\n",
    "# Read data from HDF5 file\n",
    "vx, vy, vz, UU, x_range, y_range, z_range = read_data_hdf5(input_path_hdf5)\n",
    "\n",
    "# Define trace point file paths\n",
    "trace_point_files = [\n",
    "    os.path.join(result_folder, 'best_y_trace_x-.csv'),\n",
    "    os.path.join(result_folder, 'best_y_trace_x+.csv'),\n",
    "    os.path.join(result_folder, 'best_y_trace_z-.csv'),\n",
    "    os.path.join(result_folder, 'best_y_trace_z+.csv')\n",
    "]\n",
    "\n",
    "# Colors for different trace point sets\n",
    "colors = ['lime', 'magenta', 'blue', 'red']\n",
    "\n",
    "# Create a 3D figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Read \"best_y.csv\" file and plot its points\n",
    "best_y_path = os.path.join(result_folder, 'best_y.csv')\n",
    "best_y_points = read_trace_points(best_y_path)\n",
    "\n",
    "# Read each trace point file, compute streamlines, and plot them\n",
    "for file_path, color in zip(trace_point_files, colors):\n",
    "    start_points = read_trace_points(file_path)\n",
    "    streamlines = compute_streamlines(vx, vy, vz, x_range, y_range, z_range, start_points)\n",
    "    plot_streamlines_and_points(fig, streamlines, start_points, color)\n",
    "\n",
    "# Add the best_y points to the figure as black hollow circles\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=best_y_points[:, 0], y=best_y_points[:, 1], z=best_y_points[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(color='black', size=6, symbol='circle'),\n",
    "    name='Best Y Points'\n",
    "))\n",
    "\n",
    "# Update layout to adjust display range\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(range=[0, 10000], title='X Axis'),\n",
    "        yaxis=dict(range=[0, 10000], title='Y Axis'),\n",
    "        zaxis=dict(range=[-2000, 0], title='Z Axis'),\n",
    "        aspectmode='manual',  # Allow custom aspect ratio\n",
    "        aspectratio=dict(x=1, y=1, z=0.5)  # Custom aspect ratio for display range\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=0)  # Remove margins\n",
    ")\n",
    "\n",
    "# Save the figure as an HTML file\n",
    "output_html_file = os.path.join(result_folder, '3D_GFSs_Case3S.html')\n",
    "pio.write_html(fig, file=output_html_file, auto_open=False)\n",
    "\n",
    "# Display the figure\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a20679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
