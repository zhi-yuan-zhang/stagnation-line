{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa004e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: Generate Head Dataset and Velocity Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from numba import jit, prange\n",
    "\n",
    "# Parameter settings\n",
    "params = {\n",
    "    'p': 3, 'q': 3, 'HRx': 6, 'HLx': 3, 'HRy': 3, 'HLy': 0,\n",
    "    'LX': 10000, 'LY': 10000, 'D': -2000, 'a': 2/3, 'b': 1/3,\n",
    "    'TauD': 1\n",
    "}\n",
    "\n",
    "# Extract parameters\n",
    "p, q, HRx, HLx, HRy, HLy = params['p'], params['q'], params['HRx'], params['HLx'], params['HRy'], params['HLy']\n",
    "LX, LY, D, a, b, TauD = params['LX'], params['LY'], params['D'], params['a'], params['b'], params['TauD']\n",
    "\n",
    "# Define the mesh grid\n",
    "mesh_x, mesh_y, mesh_z = 101, 101, 101\n",
    "x_range = np.linspace(0, LX, mesh_x)\n",
    "y_range = np.linspace(0, LY, mesh_y)\n",
    "z_range = np.linspace(D, 0, mesh_z)\n",
    "\n",
    "# Precompute cosine values\n",
    "b1 = np.cos(np.pi * x_range / LX)\n",
    "b2 = np.cos(p * np.pi * x_range / LX)\n",
    "b3 = np.cos(np.pi * y_range / LY)\n",
    "b4 = np.cos(q * np.pi * y_range / LY)\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def compute_J(mesh_x, mesh_y, mesh_z, x_range, y_range, z_range, HRx, HLx, HRy, HLy, LX, LY, D, a, b, tp, TauD, b1, b2, b3, b4, c1, c2, c3, c4, e1, e2):\n",
    "    \"\"\"Calculate the J matrix and related variables using the provided parameters.\"\"\"\n",
    "    J = np.zeros((mesh_x, mesh_y, mesh_z))\n",
    "    for i in prange(mesh_x):\n",
    "        for j in prange(mesh_y):\n",
    "            for k in prange(mesh_z):\n",
    "                # Calculate potential components hA, hB, and hC\n",
    "                hA = HRx + HRy - HRx * b1[i] * np.cosh(np.pi * (z_range[k] - D) / LX) / c1 - HRy * b3[j] * np.cosh(np.pi * (z_range[k] - D) / LY) / c2\n",
    "                hB = HLx + HLy - HLx * b2[i] * np.cosh(p * np.pi * (z_range[k] - D) / LX) / c3 - HLy * b4[j] * np.cosh(q * np.pi * (z_range[k] - D) / LY) / c4\n",
    "                g1, g2, g3 = 0.0, 0.0, 0.0\n",
    "                for n in range(1, 1000):  \n",
    "                    k1, an = (-1) ** (n - 1), (2 * n - 1) * np.pi / 2\n",
    "                    b0 = np.cos(an * (z_range[k] - D) / D)\n",
    "                    f1 = D * (k1 * an * b0 * (1 / an**2 + (an**2 * e1 + 2 * np.pi * TauD * e2) / (an**4 + 4 * np.pi**2 * TauD**2)))\n",
    "                    g1 += f1\n",
    "                    k2, d1 = (-1) ** n, an**2 + (p * np.pi * D / LX)**2\n",
    "                    f2 = D * (k2 * an * b0 * (1 / d1 + (d1 * e1 + 2 * np.pi * TauD * e2) / (d1**2 + 4 * np.pi**2 * TauD**2)))\n",
    "                    g2 += f2\n",
    "                    k3, d2 = (-1) ** n, an**2 + (q * np.pi * D / LY)**2\n",
    "                    f3 = D * (k3 * an * b0 * (1 / d2 + (d2 * e1 + 2 * np.pi * TauD * e2) / (d2**2 + 4 * np.pi**2 * TauD**2)))\n",
    "                    g3 += f3\n",
    "                hC = (HLx + HLy) * g1 / D + HLx * b2[i] * g2 / D + HLy * b4[j] * g3 / D\n",
    "                J[i, j, k] = hA + a * hB + b * hC\n",
    "    return J\n",
    "\n",
    "def calculate_fields(tp):\n",
    "    \"\"\"Calculate the potential field (J), velocity components (vx, vy, vz), and velocity magnitude (UU) for a given time point (tp).\"\"\"\n",
    "    e1 = np.cos(2 * np.pi * tp)\n",
    "    e2 = np.sin(2 * np.pi * tp)\n",
    "\n",
    "    # Precompute cosh values for efficiency\n",
    "    c1 = np.cosh(np.pi * D / LX)\n",
    "    c2 = np.cosh(np.pi * D / LY)\n",
    "    c3 = np.cosh(p * np.pi * D / LX)\n",
    "    c4 = np.cosh(q * np.pi * D / LY)\n",
    "\n",
    "    J = compute_J(mesh_x, mesh_y, mesh_z, x_range, y_range, z_range, HRx, HLx, HRy, HLy, LX, LY, D, a, b, tp, TauD, b1, b2, b3, b4, c1, c2, c3, c4, e1, e2)\n",
    "\n",
    "    # Compute the gradient of J\n",
    "    dx, dy, dz = x_range[1] - x_range[0], y_range[1] - y_range[0], z_range[1] - z_range[0]\n",
    "    grad_x, grad_y, grad_z = np.gradient(J, dx, dy, dz)\n",
    "\n",
    "    # Calculate the velocity components as the negative gradient of J\n",
    "    vx, vy, vz = -grad_x, -grad_y, -grad_z\n",
    "\n",
    "    # Calculate the magnitude of the velocity field\n",
    "    UU = np.sqrt(vx**2 + vy**2 + vz**2)\n",
    "\n",
    "    # Hydraulic head (H) is equivalent to J\n",
    "    H = J\n",
    "\n",
    "    return H, vx, vy, vz, UU\n",
    "\n",
    "def save_data_hdf5(filename, J, vx, vy, vz, UU, H, x_range, y_range, z_range):\n",
    "    \"\"\"Save the computed data into an HDF5 file.\"\"\"\n",
    "    with h5py.File(filename, 'w') as f:\n",
    "        f.create_dataset('J', data=J)\n",
    "        f.create_dataset('vx', data=vx)\n",
    "        f.create_dataset('vy', data=vy)\n",
    "        f.create_dataset('vz', data=vz)\n",
    "        f.create_dataset('UU', data=UU)\n",
    "        f.create_dataset('H', data=H)\n",
    "        f.create_dataset('x_range', data=x_range)\n",
    "        f.create_dataset('y_range', data=y_range)\n",
    "        f.create_dataset('z_range', data=z_range)\n",
    "\n",
    "def save_data_tecplot(filename, vx, vy, vz, UU, J, x_range, y_range, z_range):\n",
    "    \"\"\"Save the computed data into a Tecplot-compatible .dat file.\"\"\"\n",
    "    # Apply mask for z_range < -20\n",
    "    mask = z_range < -20\n",
    "\n",
    "    x, y, z = np.meshgrid(x_range, y_range, z_range, indexing='ij')\n",
    "    \n",
    "    # Flatten and filter data\n",
    "    x = x[:, :, mask].flatten()\n",
    "    y = y[:, :, mask].flatten()\n",
    "    z = z[:, :, mask].flatten()\n",
    "    vx = vx[:, :, mask].flatten()\n",
    "    vy = vy[:, :, mask].flatten()\n",
    "    vz = vz[:, :, mask].flatten()\n",
    "    UU = UU[:, :, mask].flatten()\n",
    "    J = J[:, :, mask].flatten()\n",
    "\n",
    "    # Stack data column-wise\n",
    "    data = np.column_stack((x, y, z, vx, vy, vz, UU, J))\n",
    "\n",
    "    # Sort data by x, then y, then z\n",
    "    data = data[np.lexsort((x, y, z))]\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('TITLE = \"Velocity, Speed and Head Data\"\\n')\n",
    "        f.write('VARIABLES = \"X\", \"Y\", \"Z\", \"VX\", \"VY\", \"VZ\", \"UU\", \"H\"\\n')\n",
    "        f.write(f'ZONE T=\"Flow Field\", I={len(x_range)}, J={len(y_range)}, K={len(z_range[mask])}, F=POINT\\n')\n",
    "        np.savetxt(f, data, fmt='%.6f', delimiter=' ')\n",
    "\n",
    "# Setup output directory\n",
    "current_working_dir = os.getcwd()\n",
    "result_folder = os.path.join(current_working_dir, 'results')\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "\n",
    "# Loop over time points (tp) and compute, save results\n",
    "for tp in np.arange(0, 1, 0.25):\n",
    "    H, vx, vy, vz, UU = calculate_fields(tp)\n",
    "\n",
    "    # Construct file names for the current time point\n",
    "    hdf5_filename = os.path.join(result_folder, f'head_and_velocity_tp_{tp:.2f}.h5')\n",
    "    tecplot_filename = os.path.join(result_folder, f'head_and_velocity_tp_{tp:.2f}.dat')\n",
    "    \n",
    "    # Save data to HDF5 format\n",
    "    save_data_hdf5(hdf5_filename, H, vx, vy, vz, UU, H, x_range, y_range, z_range)\n",
    "    \n",
    "    # Save data to Tecplot .dat format\n",
    "    save_data_tecplot(tecplot_filename, vx, vy, vz, UU, H, x_range, y_range, z_range)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Completed processing for tp = {tp:.2f}\")\n",
    "\n",
    "# End of processing\n",
    "print(\"All time points processed and data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdbb40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2: Finding Out (Pseudo-)stagnation Points/Lines\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.ndimage import minimum_filter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Get the current working directory\n",
    "script_dir = Path.cwd()\n",
    "\n",
    "# Define the range boundaries\n",
    "x_min, x_max = 0, 10000\n",
    "y_min, y_max = 0, 10000\n",
    "z_min, z_max = -2000, 0\n",
    "threshold = 1e-3\n",
    "\n",
    "# Define a function to filter points that are at least 200 meters apart\n",
    "def filter_points(points):\n",
    "    filtered = []\n",
    "    for point in points:\n",
    "        if all(np.linalg.norm(np.array(point) - np.array(p)) >= 200 for p in filtered):\n",
    "            filtered.append(point)\n",
    "    return filtered\n",
    "\n",
    "# Process each y-slice\n",
    "def process_slice_y(y, X_filtered, Y_filtered, Z_filtered, vx_filtered, vy_filtered, vz_filtered, UU_filtered):\n",
    "    slice_indices = np.where(Y_filtered == y)[0]\n",
    "    if len(slice_indices) == 0:\n",
    "        return []\n",
    "\n",
    "    points = np.array([X_filtered[slice_indices], Z_filtered[slice_indices]]).T\n",
    "    vx_values = vx_filtered[slice_indices]\n",
    "    vy_values = vy_filtered[slice_indices]\n",
    "    vz_values = vz_filtered[slice_indices]\n",
    "    UU_values = UU_filtered[slice_indices]\n",
    "\n",
    "    # Create a grid for interpolation\n",
    "    grid_x, grid_z = np.mgrid[x_min:x_max:200j, z_min:z_max:200j]\n",
    "    grid_vx = griddata(points, vx_values, (grid_x, grid_z), method='linear')\n",
    "    grid_vy = griddata(points, vy_values, (grid_x, grid_z), method='linear')\n",
    "    grid_vz = griddata(points, vz_values, (grid_x, grid_z), method='linear')\n",
    "    grid_UU = griddata(points, UU_values, (grid_x, grid_z), method='linear')\n",
    "\n",
    "    # Identify local minima in UU\n",
    "    UU_local_min = (grid_UU == minimum_filter(grid_UU, size=50))\n",
    "    UU_local_min_points_slice = []\n",
    "\n",
    "    # Collect points that meet the threshold condition\n",
    "    for i in range(grid_x.shape[0]):\n",
    "        for j in range(grid_x.shape[1]):\n",
    "            if UU_local_min[i, j] and x_min + 1 <= grid_x[i, j] <= x_max - 1 and z_min + 1 <= grid_z[i, j] <= z_max - 1:\n",
    "                if grid_UU[i, j] < threshold:\n",
    "                    UU_local_min_points_slice.append((grid_x[i, j], y, grid_z[i, j], grid_UU[i, j]))\n",
    "\n",
    "    return filter_points(UU_local_min_points_slice)\n",
    "\n",
    "# Aggregate and filter points from all slices\n",
    "def compare_and_filter_points(points):\n",
    "    filtered = []\n",
    "    for point in points:\n",
    "        existing_point = next((p for p in filtered if np.linalg.norm(np.array(point[:3]) - np.array(p[:3])) < 1), None)\n",
    "        if existing_point:\n",
    "            if point[3] < existing_point[3]:\n",
    "                filtered.remove(existing_point)\n",
    "                filtered.append(point)\n",
    "        else:\n",
    "            filtered.append(point)\n",
    "    return filtered\n",
    "\n",
    "# Save points to a CSV file\n",
    "def save_points_to_csv(points, file_name, header):\n",
    "    csv_path = script_dir / 'results' / file_name\n",
    "    np.savetxt(csv_path, points, delimiter=', ', header=header, comments='', fmt='%s')\n",
    "    print(f\"Found and saved {len(points)} points to {file_name}\")\n",
    "\n",
    "# Generate a .dat file from the CSV file\n",
    "def process_csv_file(file_prefix):\n",
    "    input_path = script_dir / 'results' / f'{file_prefix}.csv'\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    # Remove any spaces in column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    #print(f\"Column names: {df.columns.tolist()}\")\n",
    "\n",
    "    # Ensure required columns are present\n",
    "    required_columns = ['x', 'y', 'z']\n",
    "    actual_columns = df.columns.tolist()\n",
    "\n",
    "    if not all(col in actual_columns for col in required_columns):\n",
    "        print(f\"Error: CSV file is missing required columns. Actual columns: {actual_columns}\")\n",
    "        return\n",
    "\n",
    "    # Save the .dat file with required formatting\n",
    "    output_path_dat = script_dir / 'results' / f'{file_prefix}.dat'\n",
    "    df[['x', 'y', 'z']].to_csv(output_path_dat, sep='\\t', index=False, header=False)\n",
    "\n",
    "    with open(output_path_dat, 'r+') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write('VARIABLES = \"X\", \"Y\", \"Z\"\\n')\n",
    "        f.write(f\"ZONE T='{file_prefix}'\\n\")\n",
    "        f.write(f'I={len(df)}, J=1, K=1, F=POINT\\n')\n",
    "        f.write(content)\n",
    "    \n",
    "    print(f'File generated: {output_path_dat}')\n",
    "\n",
    "# Generate a 3D scatter plot for visualization\n",
    "def plot_points(all_points, title):\n",
    "    fig = go.Figure()\n",
    "    base_colors = ['#FF0000', '#FFA500', '#008000', '#87CEEB', '#00FFFF', '#0000FF', '#FF00CC', '#A52A2A', '#808080', '#000000']\n",
    "    \n",
    "    # Extend the color list if necessary\n",
    "    num_colors_needed = len(all_points)\n",
    "    colors = (base_colors * (num_colors_needed // len(base_colors) + 1))[:num_colors_needed]\n",
    "\n",
    "    for tp, color in zip(sorted(all_points.keys()), colors):\n",
    "        points = all_points[tp]\n",
    "        if len(points) == 0:  # Check if points list is empty\n",
    "            print(f\"No points found for tp={tp:.2f}, skipping...\")\n",
    "            continue  # Skip this iteration if no points are found\n",
    "        x, y, z, _ = zip(*points)\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            z=z,\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color=color),\n",
    "            name=f'tp={tp}'\n",
    "        ))\n",
    "\n",
    "    # Update the layout of the figure\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z',\n",
    "            xaxis=dict(range=[x_min, x_max]),\n",
    "            yaxis=dict(range=[y_min, y_max]),\n",
    "            zaxis=dict(range=[z_min, z_max]),\n",
    "            aspectmode='manual',\n",
    "            aspectratio=dict(x=1, y=1, z=0.5)\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=0)\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Main code execution\n",
    "def main():\n",
    "    all_points_y = {}\n",
    "    for tp in np.arange(0, 1, 0.25):\n",
    "        file_relative_path = Path('results', f'head_and_velocity_tp_{tp:.2f}.h5')\n",
    "        file_path = script_dir / file_relative_path\n",
    "\n",
    "        # Check if the HDF5 file exists\n",
    "        if not file_path.exists():\n",
    "            print(f\"Error: File '{file_path}' does not exist!\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Load data from the HDF5 file\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                x_range = f['x_range'][:]\n",
    "                y_range = f['y_range'][:]\n",
    "                z_range = f['z_range'][:]\n",
    "                vx = f['vx'][:]\n",
    "                vy = f['vy'][:]\n",
    "                vz = f['vz'][:]\n",
    "                UU = f['UU'][:]\n",
    "            #print(f\"File successfully loaded: {file_relative_path}\")\n",
    "        except Exception as e:\n",
    "            #print(f\"Error loading file: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Apply the filtering criteria to extract relevant data\n",
    "        X, Y, Z = np.meshgrid(x_range, y_range, z_range, indexing='ij')\n",
    "        filtered_indices = (\n",
    "            (X >= x_min) & (X <= x_max) &\n",
    "            (Y >= y_min) & (Y <= y_max) &\n",
    "            (Z >= z_min) & (Z <= z_max)\n",
    "        )\n",
    "        X_filtered = X[filtered_indices]\n",
    "        Y_filtered = Y[filtered_indices]\n",
    "        Z_filtered = Z[filtered_indices]\n",
    "        vx_filtered = vx[filtered_indices]\n",
    "        vy_filtered = vy[filtered_indices]\n",
    "        vz_filtered = vz[filtered_indices]\n",
    "        UU_filtered = UU[filtered_indices]\n",
    "\n",
    "        # Define y-slices for processing\n",
    "        y_slices = np.arange(y_min, y_max+ 1, 100)\n",
    "        UU_local_min_points_y = []\n",
    "\n",
    "        # Use parallel processing for each y-slice\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results_y = executor.map(lambda y: process_slice_y(y, X_filtered, Y_filtered, Z_filtered, vx_filtered, vy_filtered, vz_filtered, UU_filtered), y_slices)\n",
    "            for res in results_y:\n",
    "                UU_local_min_points_y.extend(res)\n",
    "\n",
    "        # Filter and save the local minima points for each time step (tp)\n",
    "        UU_local_min_points_filtered = compare_and_filter_points(UU_local_min_points_y)\n",
    "        save_points_to_csv(UU_local_min_points_filtered, f'best_y_tp_{tp:.2f}.csv', 'x, y, z, UU\\n')\n",
    "        process_csv_file(f'best_y_tp_{tp:.2f}')\n",
    "        all_points_y[tp] = UU_local_min_points_filtered\n",
    "\n",
    "    # Plot the points for all time steps\n",
    "    fig_best_y = plot_points(all_points_y, 'Best Points from Y Slices with Local Min UU')\n",
    "    output_html_file = script_dir / 'results' / 'pseudostagline_Case2T_taud_1_tsum.html'\n",
    "    pio.write_html(fig_best_y, file=output_html_file, auto_open=False)\n",
    "    fig_best_y.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7375be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the current working directory\n",
    "script_dir = Path.cwd()\n",
    "\n",
    "# Define the target directory for results\n",
    "results_dir = script_dir / 'results'\n",
    "\n",
    "# Create the results directory if it doesn't exist\n",
    "if not results_dir.exists():\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define target y-values\n",
    "y_targets = [0, 5000, 10000]\n",
    "\n",
    "# Define the range of tp values\n",
    "tp_range = np.arange(0, 1, 0.25)\n",
    "\n",
    "# Create a dictionary to store points for each y_target at different tp values\n",
    "y_points = {y: {} for y in y_targets}\n",
    "\n",
    "# Iterate through all tp values\n",
    "for tp in tp_range:\n",
    "    file_path = results_dir / f'best_y_tp_{tp:.2f}.csv'\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if not file_path.exists():\n",
    "        print(f\"Error: File '{file_path}' does not exist!\")\n",
    "        continue\n",
    "\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Filter points where y matches the target values, and x, z are within specified ranges\n",
    "    for y_target in y_targets:\n",
    "        # Use columns with leading spaces if they exist in the file\n",
    "        df_y_target = df[(df[' y'] == y_target) & (df['x'].between(4000, 6000)) & (df[' z'].between(-1000, 0))]\n",
    "        \n",
    "        # Store the filtered points\n",
    "        y_points[y_target][tp] = df_y_target[['x', ' z']].values\n",
    "\n",
    "# Define a list of colors for different tp values\n",
    "colors = [\n",
    "    '#FF0000',  # Red\n",
    "    '#FFA500',  # Orange\n",
    "    '#008000',  # Green\n",
    "    '#87CEEB',  # Sky Blue\n",
    "    '#00FFFF',  # Cyan\n",
    "    '#0000FF',  # Blue\n",
    "    '#FF00CC',  # Magenta\n",
    "    '#A52A2A',  # Brown\n",
    "    '#808080',  # Gray\n",
    "    '#000000',  # Black\n",
    "]\n",
    "\n",
    "# Create a plot for each y_target and save the data to CSV\n",
    "for y_target in y_targets:\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Collect all points\n",
    "    all_points = []\n",
    "    \n",
    "    # Iterate through each tp value and its corresponding color\n",
    "    for idx, tp in enumerate(y_points[y_target].keys()):\n",
    "        color = colors[idx % len(colors)]  # Use modulo to loop through colors\n",
    "        \n",
    "        points = y_points[y_target][tp]\n",
    "        if len(points) > 0:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=points[:, 0],  # x-coordinates\n",
    "                y=points[:, 1],  # z-coordinates\n",
    "                mode='markers',\n",
    "                marker=dict(size=5, color=color),\n",
    "                name=f'tp={tp:.2f}'\n",
    "            ))\n",
    "            \n",
    "            # Add these points to the all_points list\n",
    "            all_points.extend([(x, z, tp) for x, z in points])\n",
    "    \n",
    "    # Save the collected points to a CSV file\n",
    "    csv_file_path = results_dir / f'stagnation_point_y_{y_target}.csv'\n",
    "    df_all_points = pd.DataFrame(all_points, columns=['x', 'z', 'tp'])\n",
    "    df_all_points.to_csv(csv_file_path, index=False)\n",
    "    \n",
    "    # Configure the layout of the plot\n",
    "    fig.update_layout(\n",
    "        title=f'Points for y={y_target}',\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='Z',\n",
    "        xaxis_range=[4000, 6000],  # Set x-axis range\n",
    "        yaxis_range=[-1000, 0],    # Set z-axis range\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Save the figure as an HTML file\n",
    "    html_file_path = results_dir / f'stagnation_point_in_y{y_target}.html'\n",
    "    fig.write_html(html_file_path)\n",
    "    \n",
    "    # Display the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace4e70b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step3-1: Generate Critical Points Around (Pseudo-)Stagnation Points along 6 directions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import plotly.graph_objects as go\n",
    "from collections import namedtuple\n",
    "\n",
    "# Set up logging to track events during processing\n",
    "logging.basicConfig(filename='processing.log', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Function to read the CSV file containing best y points\n",
    "def read_best_y(file_prefix):\n",
    "    input_path = Path('results') / f'{file_prefix}.csv'\n",
    "    # Check if the file exists and is not empty\n",
    "    if not input_path.exists() or input_path.stat().st_size == 0:\n",
    "        logging.warning(f\"File {input_path} does not exist or is empty.\")\n",
    "        return None\n",
    "    return pd.read_csv(input_path)\n",
    "\n",
    "# Function to calculate the unit vector between two points\n",
    "def unit_vector(point1, point2):\n",
    "    vector = point2 - point1\n",
    "    norm = np.linalg.norm(vector)\n",
    "    if norm == 0:\n",
    "        logging.warning(f\"Zero vector detected between points {point1} and {point2}.\")\n",
    "        return np.array([0, 0, 0])\n",
    "    return vector / norm\n",
    "\n",
    "# Function to calculate the horizontal perpendicular vector\n",
    "def horizontal_perpendicular_vector(vector):\n",
    "    return np.array([-vector[1], vector[0], 0])\n",
    "\n",
    "# Function to calculate the vertical perpendicular vector\n",
    "def vertical_perpendicular_vector(_):\n",
    "    return np.array([0, 0, 1])\n",
    "\n",
    "# Function to generate tracking points based on the best y points and specified distances\n",
    "def generate_tracking_points(best_xy, distances):\n",
    "    TrackingPoints = namedtuple('TrackingPoints', ['horizon_left', 'horizon_right', 'vertical_up', 'vertical_down'])\n",
    "    tracking_points = TrackingPoints([], [], [], [])\n",
    "    \n",
    "    if len(best_xy) == 0:\n",
    "        logging.warning(\"Empty best_xy array detected.\")\n",
    "        return tracking_points._asdict()\n",
    "    \n",
    "    # Loop through best_xy to generate tracking points\n",
    "    for i in range(len(best_xy) - 1):\n",
    "        point1, point2 = best_xy[i], best_xy[i + 1]\n",
    "        direction_vector = unit_vector(point1[:3], point2[:3])\n",
    "        horizontal_unit_vector = horizontal_perpendicular_vector(direction_vector)\n",
    "        vertical_unit_vector = vertical_perpendicular_vector(direction_vector)\n",
    "        \n",
    "        tracking_points.horizon_left.append(point1[:3] - distances['horizon_left'] * horizontal_unit_vector)\n",
    "        tracking_points.horizon_right.append(point1[:3] + distances['horizon_right'] * horizontal_unit_vector)\n",
    "        tracking_points.vertical_up.append(point1[:3] + distances['vertical_up'] * vertical_unit_vector)\n",
    "        tracking_points.vertical_down.append(point1[:3] - distances['vertical_down'] * vertical_unit_vector)\n",
    "    \n",
    "    # Handle the last point separately to ensure all points are covered\n",
    "    last_point = best_xy[-1]\n",
    "    if len(best_xy) > 1:\n",
    "        previous_point = best_xy[-2]\n",
    "        direction_vector = unit_vector(last_point[:3], previous_point[:3])\n",
    "        horizontal_unit_vector = horizontal_perpendicular_vector(direction_vector)\n",
    "        vertical_unit_vector = vertical_perpendicular_vector(direction_vector)\n",
    "        \n",
    "        tracking_points.horizon_left.append(last_point[:3] + distances['horizon_left'] * horizontal_unit_vector)\n",
    "        tracking_points.horizon_right.append(last_point[:3] - distances['horizon_right'] * horizontal_unit_vector)\n",
    "        tracking_points.vertical_up.append(last_point[:3] + distances['vertical_up'] * vertical_unit_vector)\n",
    "        tracking_points.vertical_down.append(last_point[:3] - distances['vertical_down'] * vertical_unit_vector)\n",
    "    \n",
    "    return tracking_points._asdict()\n",
    "\n",
    "# Function to save the generated tracking points to a CSV file\n",
    "def save_tracking_points(filename, tracking_points):\n",
    "    df = pd.DataFrame(tracking_points, columns=['x', 'y', 'z'])\n",
    "    df.to_csv(filename, index=False)\n",
    "    logging.info(f'Tracking points saved to {filename}.')\n",
    "\n",
    "# Function to plot the best y points and tracking points in a 3D space\n",
    "def plot_points(best_xy, tracking_points, filename, tp):\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot the best y points (critical points)\n",
    "    fig.add_trace(go.Scatter3d(x=best_xy[:, 0], y=best_xy[:, 1], z=best_xy[:, 2], \n",
    "                               mode='markers', marker=dict(size=5, color='black'), name='Critical Points'))\n",
    "    \n",
    "    color_dict = {\n",
    "        'horizon_left': 'green',\n",
    "        'horizon_right': 'magenta',\n",
    "        'vertical_up': 'red',\n",
    "        'vertical_down': 'blue'\n",
    "    }\n",
    "    \n",
    "    # Plot each set of tracking points with the specified color\n",
    "    for key, points in tracking_points.items():\n",
    "        if len(points) == 0:\n",
    "            continue  # Skip if the list is empty\n",
    "        \n",
    "        points = np.array(points)\n",
    "        if points.ndim == 1:  # Skip if the array is 1-dimensional\n",
    "            continue\n",
    "\n",
    "        fig.add_trace(go.Scatter3d(x=points[:, 0], y=points[:, 1], z=points[:, 2], \n",
    "                                   mode='markers', marker=dict(size=3, color=color_dict[key]), \n",
    "                                   name=key.replace('_', ' ').title()))\n",
    "    \n",
    "    # Add tp value in the plot title\n",
    "    fig.update_layout(\n",
    "        title=f'3D Tracking Points for tp = {tp:.2f}',\n",
    "        scene=dict(\n",
    "            xaxis=dict(range=[0, 10000], title='X Axis'),\n",
    "            yaxis=dict(range=[0, 10000], title='Y Axis'),\n",
    "            zaxis=dict(range=[-2000, 0], title='Z Axis'),\n",
    "            aspectmode='manual',\n",
    "            aspectratio=dict(x=1, y=1, z=0.5)\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=0)\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    fig.write_html(str(filename))\n",
    "    logging.info(f'Plot saved as HTML: {filename} with tp = {tp:.2f}')\n",
    "\n",
    "# Function to process the CSV file and generate DAT and MCR files\n",
    "def process_csv_file(file_prefix, color):\n",
    "    input_path = Path('results') / f'{file_prefix}.csv'\n",
    "    try:\n",
    "        df = pd.read_csv(input_path)\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {input_path}\")\n",
    "        return\n",
    "    \n",
    "    output_path_dat = Path('results') / f'{file_prefix}.dat'\n",
    "    df[['x', 'y', 'z']].to_csv(output_path_dat, sep='\\t', index=False, header=False)\n",
    "    \n",
    "    # Add header information to the DAT file\n",
    "    with open(output_path_dat, 'r+') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write('VARIABLES = \"X\", \"Y\", \"Z\"\\n')\n",
    "        f.write(f\"ZONE T='{file_prefix}'\\n\")\n",
    "        f.write(f'I={len(df)}, J=1, K=1, F=POINT\\n')\n",
    "        f.write(content)\n",
    "    \n",
    "    # Generate the corresponding macro file for visualization\n",
    "    generate_macro_file(df, file_prefix, color)\n",
    "    logging.info(f'Generated files: {output_path_dat}, {file_prefix}.mcr')\n",
    "\n",
    "# Function to generate a macro file for visualization\n",
    "def generate_macro_file(df, file_prefix, color):\n",
    "    macro_file_path = Path('results') / f'{file_prefix}.mcr'\n",
    "    points = df[['x', 'y', 'z']].values.tolist()\n",
    "    \n",
    "    with open(macro_file_path, mode='w') as macrofile:\n",
    "        macrofile.write('#!MC 1410\\n')\n",
    "        macrofile.write(f'$!StreamAttributes Color = blue\\n')\n",
    "        for point in points:\n",
    "            macrofile.write('$!Streamtrace Add\\n')\n",
    "            macrofile.write('  StreamType = VolumeLine\\n')\n",
    "            macrofile.write('  StreamDirection = Both\\n')\n",
    "            macrofile.write('  StartPos\\n')\n",
    "            macrofile.write('    {\\n')\n",
    "            macrofile.write(f'    X = {point[0]}\\n')\n",
    "            macrofile.write(f'    Y = {point[1]}\\n')\n",
    "            macrofile.write(f'    Z = {point[2]}\\n')\n",
    "            macrofile.write('    }\\n')\n",
    "        macrofile.write('$!REDRAWALL\\n')\n",
    "    logging.info(f'Macro file saved: {macro_file_path}')\n",
    "\n",
    "# Main function to process all files and generate outputs\n",
    "def main():\n",
    "    result_folder = Path(os.getcwd()) / 'results'\n",
    "    tracking_distances = {\n",
    "        'horizon_left': 500,\n",
    "        'horizon_right': 500,\n",
    "        'vertical_up': 150,\n",
    "        'vertical_down': 300\n",
    "    }\n",
    "\n",
    "    # Loop through different tp values to process each corresponding file\n",
    "    for tp in np.arange(0, 1, 0.25):\n",
    "        file_prefix = f'best_y_tp_{tp:.2f}'\n",
    "        best_y_data = read_best_y(file_prefix)\n",
    "        if best_y_data is None:\n",
    "            continue\n",
    "        \n",
    "        tracking_points = generate_tracking_points(best_y_data.to_numpy(), tracking_distances)\n",
    "        \n",
    "        # Save tracking points to individual CSV files\n",
    "        for prefix, points in tracking_points.items():\n",
    "            save_tracking_points(result_folder / f'best_y_trace_{prefix}_tp_{tp:.2f}.csv', points)\n",
    "        \n",
    "        # Print the current tp value for tracking\n",
    "        print(f'Processing tp = {tp:.2f}')\n",
    "        \n",
    "        # Plot the points and save the plot as an HTML file\n",
    "        #plot_points(best_y_data.to_numpy(), tracking_points, result_folder / f'tracking_points_tp_{tp:.2f}.html', tp)\n",
    "\n",
    "        # Generate DAT and MCR files for each tracking point category\n",
    "        for category, color in [('horizon_left', 2), ('horizon_right', 3), ('vertical_up', 4), ('vertical_down', 5)]:\n",
    "            process_csv_file(f'best_y_trace_{category}_tp_{tp:.2f}', color)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d7f2a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 4: Merge Dividing Streamlines and Show Groundwater Flow Systems\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from scipy.integrate import solve_ivp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from numba import jit\n",
    "import plotly.io as pio\n",
    "\n",
    "# Function to read data from an HDF5 file\n",
    "def read_data_hdf5(filename):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        # Read velocity components and other relevant datasets from the file\n",
    "        vx = f['vx'][:]         # Velocity component in x-direction\n",
    "        vy = f['vy'][:]         # Velocity component in y-direction\n",
    "        vz = f['vz'][:]         # Velocity component in z-direction\n",
    "        UU = f['UU'][:]         # Additional dataset (context-specific)\n",
    "        x_range = f['x_range'][:]  # Range of x values\n",
    "        y_range = f['y_range'][:]  # Range of y values\n",
    "        z_range = f['z_range'][:]  # Range of z values\n",
    "    return vx, vy, vz, UU, x_range, y_range, z_range\n",
    "\n",
    "# JIT-compiled function to compute the velocity field at a given position\n",
    "@jit(nopython=True)\n",
    "def velocity_field_numba(pos, vx, vy, vz, x_range, y_range, z_range):\n",
    "    x, y, z = pos\n",
    "    # Check if the position is outside the defined ranges\n",
    "    if x < x_range[0] or x > x_range[-1] or y < y_range[0] or y > y_range[-1] or z < z_range[0] or z > z_range[-1]:\n",
    "        return np.array([0.0, 0.0, 0.0])  # Return zero velocity if out of bounds\n",
    "    \n",
    "    # Find the indices of the grid cell containing the position\n",
    "    xi = np.searchsorted(x_range, x) - 1\n",
    "    yi = np.searchsorted(y_range, y) - 1\n",
    "    zi = np.searchsorted(z_range, z) - 1\n",
    "\n",
    "    # Get the surrounding grid points\n",
    "    x1, x2 = x_range[xi], x_range[xi+1]\n",
    "    y1, y2 = y_range[yi], y_range[yi+1]\n",
    "    z1, z2 = z_range[zi], z_range[zi+1]\n",
    "\n",
    "    # Compute relative distances within the grid cell\n",
    "    xd = (x - x1) / (x2 - x1)\n",
    "    yd = (y - y1) / (y2 - y1)\n",
    "    zd = (z - z1) / (z2 - z1)\n",
    "\n",
    "    # Trilinear interpolation for the x-component of velocity\n",
    "    c00 = vx[xi, yi, zi] * (1 - xd) + vx[xi + 1, yi, zi] * xd\n",
    "    c01 = vx[xi, yi, zi + 1] * (1 - xd) + vx[xi + 1, yi, zi + 1] * xd\n",
    "    c10 = vx[xi, yi + 1, zi] * (1 - xd) + vx[xi + 1, yi + 1, zi] * xd\n",
    "    c11 = vx[xi, yi + 1, zi + 1] * (1 - xd) + vx[xi + 1, yi + 1, zi + 1] * xd\n",
    "\n",
    "    c0 = c00 * (1 - yd) + c10 * yd\n",
    "    c1 = c01 * (1 - yd) + c11 * yd\n",
    "\n",
    "    vx_val = c0 * (1 - zd) + c1 * zd  # Final interpolated x-velocity\n",
    "\n",
    "    # Trilinear interpolation for the y-component of velocity\n",
    "    c00 = vy[xi, yi, zi] * (1 - xd) + vy[xi + 1, yi, zi] * xd\n",
    "    c01 = vy[xi, yi, zi + 1] * (1 - xd) + vy[xi + 1, yi, zi + 1] * xd\n",
    "    c10 = vy[xi, yi + 1, zi] * (1 - xd) + vy[xi + 1, yi + 1, zi] * xd\n",
    "    c11 = vy[xi, yi + 1, zi + 1] * (1 - xd) + vy[xi + 1, yi + 1, zi + 1] * xd\n",
    "\n",
    "    c0 = c00 * (1 - yd) + c10 * yd\n",
    "    c1 = c01 * (1 - yd) + c11 * yd\n",
    "\n",
    "    vy_val = c0 * (1 - zd) + c1 * zd  # Final interpolated y-velocity\n",
    "\n",
    "    # Trilinear interpolation for the z-component of velocity\n",
    "    c00 = vz[xi, yi, zi] * (1 - xd) + vz[xi + 1, yi, zi] * xd\n",
    "    c01 = vz[xi, yi, zi + 1] * (1 - xd) + vz[xi + 1, yi, zi + 1] * xd\n",
    "    c10 = vz[xi, yi + 1, zi] * (1 - xd) + vz[xi + 1, yi + 1, zi] * xd\n",
    "    c11 = vz[xi, yi + 1, zi + 1] * (1 - xd) + vz[xi + 1, yi + 1, zi + 1] * xd\n",
    "\n",
    "    c0 = c00 * (1 - yd) + c10 * yd\n",
    "    c1 = c01 * (1 - yd) + c11 * yd\n",
    "\n",
    "    vz_val = c0 * (1 - zd) + c1 * zd  # Final interpolated z-velocity\n",
    "\n",
    "    return np.array([vx_val, vy_val, vz_val])  # Return the interpolated velocity vector\n",
    "\n",
    "# Function to compute streamlines based on velocity fields and start points\n",
    "def compute_streamlines(vx, vy, vz, x_range, y_range, z_range, start_points, max_distance=1e7, tol=1e-7):\n",
    "    # Define the velocity field function for the ODE solver\n",
    "    def velocity_field(t, pos):\n",
    "        return velocity_field_numba(pos, vx, vy, vz, x_range, y_range, z_range)\n",
    "\n",
    "    # Define an event to terminate integration when a boundary condition is met\n",
    "    def boundary_event(t, pos):\n",
    "        return pos[2] + 40  # Termination condition, e.g., z = -80 surface\n",
    "\n",
    "    boundary_event.terminal = True  # Stop integration when event is triggered\n",
    "    boundary_event.direction = 0     # Event is detected regardless of direction\n",
    "\n",
    "    # Function to integrate the streamline starting from a given point\n",
    "    def integrate_streamline(start):\n",
    "        # Integrate forward in time\n",
    "        result_forward = solve_ivp(velocity_field, [0, max_distance], start, method='RK45', rtol=tol, atol=tol, events=boundary_event)\n",
    "        \n",
    "        # Integrate backward in time\n",
    "        result_backward = solve_ivp(velocity_field, [0, -max_distance], start, method='RK45', rtol=tol, atol=tol, events=boundary_event)\n",
    "        \n",
    "        # Extract the streamline points\n",
    "        streamline_forward = result_forward.y.T\n",
    "        streamline_backward = result_backward.y.T\n",
    "        \n",
    "        # Reverse the backward streamline and combine with forward streamline\n",
    "        streamline_backward = streamline_backward[::-1]\n",
    "        streamline = np.vstack((streamline_backward, streamline_forward))\n",
    "        \n",
    "        return streamline\n",
    "\n",
    "    # Use a thread pool to parallelize streamline computations\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        streamlines = list(executor.map(integrate_streamline, start_points))\n",
    "    \n",
    "    return streamlines  # Return the list of computed streamlines\n",
    "\n",
    "# Function to plot streamlines and their corresponding start points\n",
    "def plot_streamlines_and_points(fig, streamlines, start_points, color):\n",
    "    for streamline in streamlines:\n",
    "        if len(streamline) > 0:\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=streamline[:, 0], y=streamline[:, 1], z=streamline[:, 2],\n",
    "                mode='lines',\n",
    "                line=dict(color=color, width=4)  # Set streamline color and width\n",
    "            ))\n",
    "    \n",
    "    # Plot the start points of the streamlines\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=start_points[:, 0], y=start_points[:, 1], z=start_points[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(color=color, size=5, symbol='square'),  # Set marker color, size, and shape\n",
    "        name=f'Start Points {color}'\n",
    "    ))\n",
    "\n",
    "# Function to read trace points from a CSV file\n",
    "def read_trace_points(file_path):\n",
    "    # Read the first few rows to determine the number of columns\n",
    "    df = pd.read_csv(file_path, nrows=5)\n",
    "    \n",
    "    # Check the number of columns and read the full file accordingly\n",
    "    if df.shape[1] == 4:\n",
    "        df = pd.read_csv(file_path, names=['x', 'y', 'z', 'UU'], header=0, skiprows=0)\n",
    "    elif df.shape[1] == 3:\n",
    "        df = pd.read_csv(file_path, names=['x', 'y', 'z'], header=0, skiprows=0)\n",
    "    else:\n",
    "        raise ValueError(\"CSV file format is incorrect. It should contain 3 or 4 columns.\")\n",
    "    \n",
    "    return df[['x', 'y', 'z']].values  # Return only the x, y, z coordinates\n",
    "\n",
    "# Get the current working directory as the base path for the script\n",
    "current_working_dir = os.getcwd()\n",
    "\n",
    "# Define the path to the results folder and create it if it doesn't exist\n",
    "result_folder = os.path.join(current_working_dir, 'results')\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "\n",
    "# Define a list of colors to be used for different tracking point categories\n",
    "colors = [ 'magenta','green', 'red', 'blue']\n",
    "\n",
    "# Define prefixes for the tracking point files\n",
    "tracking_point_prefixes = [\n",
    "    'best_y_trace_horizon_left',\n",
    "    'best_y_trace_horizon_right',\n",
    "    'best_y_trace_vertical_up',\n",
    "    'best_y_trace_vertical_down'\n",
    "]\n",
    "\n",
    "# Function to read the \"best_y.csv\" file containing points of interest\n",
    "def read_best_y(file_prefix):\n",
    "    best_y_path = os.path.join(result_folder, f'{file_prefix}.csv')\n",
    "    df = pd.read_csv(best_y_path, header=0, skiprows=0)\n",
    "    return df\n",
    "\n",
    "# Loop through different tp values to process each corresponding file\n",
    "for tp in np.arange(0, 1, 0.25):  # Iterate over tp from 0 to 0.9 with step 0.1\n",
    "    # Define the path to the input HDF5 file for the current tp value\n",
    "    input_path_hdf5 = os.path.join(result_folder, f'head_and_velocity_tp_{tp:.2f}.h5')\n",
    "    \n",
    "    # Read data from the HDF5 file\n",
    "    vx, vy, vz, UU, x_range, y_range, z_range = read_data_hdf5(input_path_hdf5)\n",
    "\n",
    "    # Create a new Plotly figure for visualization\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Loop through each tracking point category and its corresponding color\n",
    "    for prefix, color in zip(tracking_point_prefixes, colors):\n",
    "        # Define the path to the tracking point CSV file\n",
    "        file_path = os.path.join(result_folder, f'{prefix}_tp_{tp:.2f}.csv')\n",
    "        # Read the start points for streamlines\n",
    "        start_points = read_trace_points(file_path)\n",
    "        \n",
    "        # Compute streamlines based on the velocity field and start points\n",
    "        streamlines = compute_streamlines(vx, vy, vz, x_range, y_range, z_range, start_points)\n",
    "        \n",
    "        # Plot the computed streamlines and their start points on the figure\n",
    "        plot_streamlines_and_points(fig, streamlines, start_points, color)\n",
    "\n",
    "    # Define the path to the \"best_y\" points CSV file for the current tp value\n",
    "    best_y_path = os.path.join(result_folder, f'best_y_tp_{tp:.2f}.csv')\n",
    "    # Read the best y points\n",
    "    best_y_points = read_trace_points(best_y_path)\n",
    "\n",
    "    # Add the best y points to the plot as black hollow circles\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=best_y_points[:, 0], y=best_y_points[:, 1], z=best_y_points[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(color='black', size=6, symbol='circle'),  # Black hollow circles\n",
    "        name='Best Y Points'\n",
    "    ))\n",
    "\n",
    "    # Update the layout of the plot to set axis ranges and aspect ratios\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(range=[0, 10000], title='X Axis'),  # Set X-axis range and title\n",
    "            yaxis=dict(range=[0, 10000], title='Y Axis'),  # Set Y-axis range and title\n",
    "            zaxis=dict(range=[-2000, 0], title='Z Axis'),  # Set Z-axis range and title\n",
    "            aspectmode='manual',  # Enable manual aspect ratio settings\n",
    "            aspectratio=dict(x=1, y=1, z=0.5)  # Define custom aspect ratios\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=0)  # Remove margins around the plot\n",
    "    )\n",
    "    \n",
    "    # Print the current tp value for tracking\n",
    "    print(f'Processing tp = {tp:.2f}')\n",
    "    \n",
    "    # Define the path to save the output HTML file for the current tp value\n",
    "    output_html_file = os.path.join(result_folder, f'3D_GFSs_Case2T_taud_1_tp_{tp:.2f}.html')\n",
    "    # Save the figure as an HTML file without automatically opening it\n",
    "    pio.write_html(fig, file=output_html_file, auto_open=False)\n",
    "\n",
    "    # Display the figure\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d33d485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42307c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
